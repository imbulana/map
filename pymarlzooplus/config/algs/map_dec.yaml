# --- MAT-Dec specific parameters ---

action_selector: "soft_policies"
mask_before_softmax: True                           

runner: "parallel"

buffer_size: 1 # 50
batch_size_run: 1 # 50
batch_size: 1 # 50

# update the target network every {} training steps
target_update_interval_or_tau: 200

obs_agent_id: True
obs_last_action: False # It should be False
obs_individual_obs: False                           

mac: "map_mac"
agent: "mlp_map"
agent_output_type: "pi_logits"                      
learner: "map_learner"
use_rnn: False  # Should be False
standardise_returns: False  # Should be False
standardise_rewards: False  # Should be False
critic_type: "map_critic"
name: "map_dec"

#     network parameters
use_popart: True                       # help="by default False, use PopArt to normalize rewards."

#     optimizer parameters
lr: 5e-4                                # help='learning rate (default: 5e-4)'
opti_eps: 1e-5                          # help='RMSprop optimizer epsilon (default: 1e-5)'
weight_decay: 0  

#     ppo parameters
ppo_epoch: 15                           # help='number of ppo epochs (default: 15)'
use_clipped_value_loss: True            # help="by default, clip loss value. If set, do not clip loss value."
clip_param: 0.2                         # !!same as eps_clip parameter!! help='ppo clip parameter (default: 0.2)'   # 
num_mini_batch: 1                       # help='number of batches for ppo (default: 1)'
entropy_coef: 0.01                      # help='entropy term coefficient (default: 0.01)'
value_loss_coef: 1                      # help='value loss coefficient (default: 0.5)'
use_max_grad_norm: True                 # help="by default, use max norm of gradients. If set, do not use."
max_grad_norm: 10.0                     # help='max norm of gradients (default: 0.5)'
use_gae: True                           # help='use generalized advantage estimation'
gamma: 0.99                             # help='discount factor for rewards (default: 0.99)'
gae_lambda: 0.95                        # help='gae lambda parameter (default: 0.95)'
use_huber_loss: True                    # help="by default, use huber loss. If set, do not use huber loss."
huber_delta: 10.0                       # help=" coefficience of huber loss."

# transformer
n_block: 1
n_embd: 64
n_head: 1
# local_attn_scales: []
# use_global_attn_head: True
# use_perceiver_encoder: True
# perceiver_depth: 1
# perceiver_num_latents: 4 # match typical n_agents for simple encoder behaviour
# perceiver_latent_dim: 64 # align latent width with n_embd
# perceiver_latent_heads: 1 # mirror encoder head count
# perceiver_cross_heads: 1 # keep cross-attn identical to encoder
# perceiver_self_per_cross: 1
# perceiver_latent_dim_head: 64 # latent_heads * head_dim = latent_dim
# perceiver_cross_dim_head: 64 # keep cross head dim equal to latent head dim
# perceiver_agent_heads: 1
# perceiver_dropout: 0.

# buffer
extra_in_buffer: ["log_probs", "values"]

# perceiver_depth: 1
# perceiver_num_latents: 16
# perceiver_latent_heads: 4
# perceiver_cross_heads: 1
# perceiver_self_per_cross_attn: 2
# perceiver_latent_dim_head: 8      # latent_heads * latent_dim_head = n_embd
# perceiver_cross_dim_head: 8
# perceiver_num_freq_bands: 6
# perceiver_max_freq: 10.0
# perceiver_input_axis: 1
# perceiver_attn_dropout: 0.0
# perceiver_ff_dropout: 0.0
# perceiver_weight_tie_layers: False
# perceiver_fourier_encode_data: False
